#!/usr/bin/env zsh

# AI Command Line Helper using Ollama
# Usage: ai-helper [model_name] or source this file and use 'ai' function

# Default model - can be overridden by environment variable or command line argument
DEFAULT_MODEL="${OLLAMA_MODEL:-llama3.2}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to check if ollama is installed and running
check_ollama() {
    if ! command -v ollama &> /dev/null; then
        echo -e "${RED}Error: Ollama is not installed or not in PATH${NC}"
        echo "Please install Ollama from https://ollama.ai"
        return 1
    fi
    
    # Check if ollama service is running
    if ! ollama list &> /dev/null; then
        echo -e "${YELLOW}Warning: Ollama service might not be running${NC}"
        echo "Try running: ollama serve"
        return 1
    fi
    
    return 0
}

# Function to get recent command history
get_command_history() {
    # Get last 20 commands from current session, excluding the ai helper calls
    # Handle case where there's no history available
    local history_output
    history_output=$(fc -l -20 2>/dev/null | grep -v "ai " | grep -v "ai-helper" | tail -10 2>/dev/null)
    
    if [ -z "$history_output" ]; then
        echo "No recent command history available"
    else
        echo "$history_output"
    fi
}

# Function to generate command using Ollama
generate_command() {
    local prompt="$1"
    local model="$2"
    local history="$3"
    
    # Create the system prompt
    local system_prompt="You are a helpful command line assistant. Generate ONLY the exact terminal command needed to accomplish the user's request. Do not include explanations, markdown formatting, or any other text - just the raw command that can be executed directly in a terminal.

Context: The user is on macOS (darwin) using zsh shell.

Recent command history for context:
$history

User request: $prompt

Respond with ONLY the command, nothing else."

    # Call Ollama API
    local response=$(ollama run "$model" "$system_prompt" 2>/dev/null)
    
    if [ $? -ne 0 ]; then
        echo -e "${RED}Error: Failed to get response from Ollama${NC}"
        return 1
    fi
    
    # Clean up the response - remove any potential markdown or extra formatting
    echo "$response" | sed 's/```.*//g' | sed 's/`//g' | xargs
}

# Function to confirm and execute command
confirm_and_execute() {
    local command="$1"
    
    echo -e "${BLUE}Generated command:${NC}"
    echo -e "${GREEN}$command${NC}"
    echo
    
    # Ask for confirmation
    echo -n -e "${YELLOW}Execute this command? [y/N]: ${NC}"
    read -r response
    
    case "$response" in
        [yY]|[yY][eE][sS])
            echo -e "${GREEN}Executing...${NC}"
            eval "$command"
            ;;
        *)
            echo -e "${YELLOW}Command not executed.${NC}"
            ;;
    esac
}

# Main AI helper function
ai() {
    local model="$DEFAULT_MODEL"
    local prompt=""
    
    # Parse arguments
    if [ $# -eq 0 ]; then
        echo -e "${YELLOW}Usage: ai [model_name] <prompt>${NC}"
        echo -e "${YELLOW}   or: ai <prompt>${NC}"
        echo -e "${YELLOW}Current model: $model${NC}"
        return 1
    fi
    
    # Check if first argument is a model name (check if it exists in ollama list)
    if [ $# -gt 1 ] && [[ "$1" =~ ^[a-zA-Z0-9._:-]+$ ]] && ollama list | grep -q "^$1"; then
        model="$1"
        shift
        prompt="$*"
    else
        prompt="$*"
    fi
    
    if [ -z "$prompt" ]; then
        echo -e "${RED}Error: No prompt provided${NC}"
        return 1
    fi
    
    # Check ollama availability
    if ! check_ollama; then
        return 1
    fi
    
    echo -e "${BLUE}Using model: $model${NC}"
    echo -e "${BLUE}Prompt: $prompt${NC}"
    echo
    
    # Get command history
    local history=$(get_command_history)
    
    # Generate command
    echo -e "${YELLOW}Generating command...${NC}"
    local generated_command=$(generate_command "$prompt" "$model" "$history")
    
    if [ $? -ne 0 ] || [ -z "$generated_command" ]; then
        echo -e "${RED}Error: Failed to generate command${NC}"
        return 1
    fi
    
    # Confirm and execute
    confirm_and_execute "$generated_command"
}

# Main execution logic
main() {
    # Parse command line arguments for direct execution
    if [ $# -eq 0 ]; then
        echo -e "${YELLOW}AI Command Line Helper${NC}"
        echo -e "${YELLOW}Usage: $SCRIPT_NAME [model_name] <prompt>${NC}"
        echo -e "${YELLOW}   or: source $SCRIPT_NAME and use 'ai' function${NC}"
        echo -e "${YELLOW}Current default model: $DEFAULT_MODEL${NC}"
        echo
        echo -e "${YELLOW}Examples:${NC}"
        echo -e "  $SCRIPT_NAME kill the process using port 8080"
        echo -e "  $SCRIPT_NAME llama3.1 find all .js files modified in last 24 hours"
        echo -e "  source $SCRIPT_NAME && ai list all docker containers"
        return 1
    fi
    
    # Run the ai function with all arguments
    ai "$@"
}

# If script is being run directly (not sourced)
if [[ "$ZSH_EVAL_CONTEXT" == "toplevel" ]]; then
    # Store script name for use in help text
    SCRIPT_NAME="$0"
    # Run main function with all arguments
    main "$@"
else
    # Script is being sourced
    echo -e "${GREEN}AI Command Line Helper loaded!${NC}"
    echo -e "${YELLOW}Usage: ai [model_name] <prompt>${NC}"
    echo -e "${YELLOW}Current default model: $DEFAULT_MODEL${NC}"
    echo -e "${YELLOW}Set OLLAMA_MODEL environment variable to change default model${NC}"
fi 